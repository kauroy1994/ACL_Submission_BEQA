{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Library Installations**"
      ],
      "metadata": {
        "id": "ATjG5tXJyiWp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV_8intHyg22"
      },
      "outputs": [],
      "source": [
        "!pip install datasets pydantic beautifulsoup4\n",
        "!pip install sentence-transformers\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "!pip install rouge_score\n",
        "!pip install datasets\n",
        "!pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Dataset Preparation**"
      ],
      "metadata": {
        "id": "-a-AWnwnzfrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from bs4 import BeautifulSoup\n",
        "from random import choice\n",
        "\n",
        "def clean_html(html_content: str) -> str:\n",
        "    \"\"\"Clean HTML content by removing HTML tags.\"\"\"\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "        return soup.get_text(strip=True)\n",
        "    return \"\"\n",
        "\n",
        "def clean_description(description: list) -> str:\n",
        "    \"\"\"Clean description list into a single string.\"\"\"\n",
        "    if description:\n",
        "        return \" \".join(description)  # Join list items into a single string\n",
        "    return \"\"\n",
        "\n",
        "def stream_clean_rag_samples(dataset_name: str, dataset_config: str, num_samples: int):\n",
        "\n",
        "    #placeholder for the data\n",
        "    data = []\n",
        "\n",
        "    # Load the dataset from Hugging Face with streaming enabled and a specific configuration\n",
        "    dataset = load_dataset(dataset_name, dataset_config, split=\"train\", streaming=True)  # Adjust the split if necessary\n",
        "\n",
        "    # Stream random samples\n",
        "    count = 0\n",
        "    for sample in dataset:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # Extract question (handling 'text' subfield if present)\n",
        "        question = sample.get(\"question\", None)\n",
        "        if isinstance(question, dict):\n",
        "            question = question.get(\"text\", None)\n",
        "\n",
        "        context = sample.get(\"document_title\", None) or sample.get(\"context\", None) or sample.get(\"document\", None) or sample.get(\"search_results\", None)\n",
        "\n",
        "        # Clean HTML or description if the context contains the relevant subfields\n",
        "        if context and isinstance(context, dict):\n",
        "            if \"html\" in context:\n",
        "                context = clean_html(context[\"html\"])  # Clean the HTML content to plain text\n",
        "            elif \"description\" in context:\n",
        "                context = clean_description(context[\"description\"])  # Convert the list to a plain string\n",
        "\n",
        "        # Handle response, checking for different possible field names\n",
        "        try:\n",
        "            response = choice(sample.get(\"answer\", {}).get(\"aliases\", [None])) or sample.get(\"long_answer_candidates\", None) or sample.get(\"answers\", {}).get(\"text\", [None])[0]\n",
        "\n",
        "            # If response is a list or dict, extract the answer text\n",
        "            if isinstance(response, list):\n",
        "              response = response[0] if response else None  # Take the first answer if it's a list\n",
        "            elif isinstance(response, dict):\n",
        "              response = response.get(\"text\", None) or response.get(\"value\", None) # Get value from 'text' or 'value' field\n",
        "\n",
        "        except:\n",
        "            response = sample.get(\"answer\", None) or sample.get(\"long_answer_candidates\", None) or sample.get(\"answers\", {}).get(\"text\", [None])[0]\n",
        "\n",
        "\n",
        "        # Check if any field is missing and print the actual field names in the sample\n",
        "        missing_fields = []\n",
        "        if question is None:\n",
        "            missing_fields.append(\"question\")\n",
        "        if context is None:\n",
        "            missing_fields.append(\"context/document/search_results\")\n",
        "        if response is None:\n",
        "            missing_fields.append(\"response/long_answer_candidates/answer\")\n",
        "\n",
        "        if missing_fields:\n",
        "            # Print the actual field names present in the sample\n",
        "            print(f\"Sample {count + 1} - Present fields: {', '.join(sample.keys())}\")\n",
        "\n",
        "        # Clean output\n",
        "        #print(f\"Sample {count + 1}:\")\n",
        "        #print(f\"Question: {question if question else 'No question found.'}\")\n",
        "        #print(f\"Context: {context if context else 'No context found.'}\")\n",
        "        #print(f\"Response: {response if response else 'No response found.'}\")\n",
        "        #print(\"-\" * 50)\n",
        "\n",
        "        data.append({\"Question\": question, \"Context\": context, \"Response\": response})\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage: Streaming from SQuAD, Trivia QA, and WikiQA datasets\n",
        "datasets = [\n",
        "    (\"squad\", None),  # SQuAD dataset\n",
        "    (\"trivia_qa\", \"rc\"),  # Config is required for trivia_qa\n",
        "    (\"wiki_qa\", None)  # WikiQA dataset\n",
        "]\n",
        "\n",
        "full_data = {}\n",
        "\n",
        "for dataset_name, dataset_config in datasets:\n",
        "    print(f\"Streaming from dataset: {dataset_name}\")\n",
        "    full_data[dataset_name] = stream_clean_rag_samples(dataset_name, dataset_config, num_samples=5)\n",
        "    print(\"=\" * 100)"
      ],
      "metadata": {
        "id": "GYk1z6yIzkDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Ratio of Improvement Metric**"
      ],
      "metadata": {
        "id": "6Q4wbFvw0q72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def ratio_of_improvement(sim_beqa, sim_baseline):\n",
        "    \"\"\"\n",
        "    Computes the Ratio of Improvement (RI) for BEQA vs. a baseline approach.\n",
        "\n",
        "    RI is defined as:\n",
        "\n",
        "      RI = ( Σ Sim_BEQA(q_m) ) / ( Σ Sim_Baseline(q_m) )\n",
        "\n",
        "    where the sums are taken over all queries m in {1, 2, ..., M}.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sim_beqa : list or np.ndarray\n",
        "        A list/array of semantic similarity scores for queries using the BEQA approach.\n",
        "    sim_baseline : list or np.ndarray\n",
        "        A list/array of semantic similarity scores for queries using the baseline approach.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The ratio of the sum of BEQA similarity scores to the sum of baseline similarity scores.\n",
        "        A value > 1.0 indicates that BEQA outperforms the baseline on average.\n",
        "    \"\"\"\n",
        "    sim_beqa = np.array(sim_beqa, dtype=float)\n",
        "    sim_baseline = np.array(sim_baseline, dtype=float)\n",
        "\n",
        "    # Safety checks\n",
        "    if len(sim_beqa) == 0 or len(sim_baseline) == 0:\n",
        "        raise ValueError(\"Input lists cannot be empty.\")\n",
        "    if len(sim_beqa) != len(sim_baseline):\n",
        "        raise ValueError(\"The two lists must have the same length.\")\n",
        "\n",
        "    sum_beqa = np.sum(sim_beqa)\n",
        "    sum_baseline = np.sum(sim_baseline)\n",
        "\n",
        "    # Avoid division by zero in edge cases\n",
        "    if sum_baseline == 0:\n",
        "        raise ZeroDivisionError(\"The sum of baseline similarities is zero.\")\n",
        "\n",
        "    return sum_beqa / sum_baseline\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Suppose we have similarity scores for 5 queries\n",
        "    sim_beqa_example = [0.82, 0.91, 0.78, 0.88, 0.85]\n",
        "    sim_baseline_example = [0.80, 0.85, 0.74, 0.89, 0.80]\n",
        "\n",
        "    ri_value = ratio_of_improvement(sim_beqa_example, sim_baseline_example)\n",
        "    print(f\"Ratio of Improvement (RI) = {ri_value:.3f}\")"
      ],
      "metadata": {
        "id": "GMeFG1uR0vXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. TF-IDF with BEQA**"
      ],
      "metadata": {
        "id": "QeSgccys75da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "from typing import List, Dict\n",
        "from pydantic import BaseModel, ValidationError, Field\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. Existing Pydantic Model + BEQA Transform Code (Groq)\n",
        "# -------------------------------------------------------------------\n",
        "class QAItem(BaseModel):\n",
        "    Question: str = Field(..., description=\"The transformed question\")\n",
        "    Answer: str = Field(..., description=\"The transformed answer\")\n",
        "\n",
        "THROTTLE_LIMIT = 3  # Seconds between consecutive API calls\n",
        "MAX_RETRIES = 5     # Maximum number of retries for valid responses\n",
        "\n",
        "GROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"  # Example endpoint\n",
        "\n",
        "def parse_groq_output(raw_output: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parse and validate the output from the Groq model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = json.loads(raw_output)\n",
        "        if not isinstance(data, list):\n",
        "            print(\"Parsed data is not a list.\")\n",
        "            return []\n",
        "        items = []\n",
        "        for item in data:\n",
        "            qa_item = QAItem(**item)  # Validate via Pydantic\n",
        "            items.append(qa_item.dict())\n",
        "        return items\n",
        "\n",
        "    except (ValidationError, json.JSONDecodeError, TypeError) as e:\n",
        "        print(f\"Failed to parse or validate JSON output: {e}\")\n",
        "        return []\n",
        "\n",
        "def transform_with_groq(context: str, original_query: str, model_id: str, api_key: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Transform retrieved context into structured QA pairs using the Groq API with retries.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "    }\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "You are given:\n",
        "- Context: \"{context}\"\n",
        "- Original Query: \"{original_query}\"\n",
        "\n",
        "Your task:\n",
        "1. Generate one or more question-answer pairs that capture key information from the context.\n",
        "2. Output your result as valid JSON (no markdown) and use the following schema (an array of objects):\n",
        "   [\n",
        "     {{\n",
        "       \"Question\": \"...\",\n",
        "       \"Answer\": \"...\"\n",
        "     }},\n",
        "     ...\n",
        "   ]\n",
        "\n",
        "Constraints:\n",
        "- Make sure the JSON is valid.\n",
        "- No additional keys beyond 'Question' and 'Answer'.\n",
        "- Do not wrap the output in quotes or markdown.\n",
        "- If you are unsure, keep it simple.\n",
        "\"\"\"\n",
        "\n",
        "    retries = 0\n",
        "    while retries < MAX_RETRIES:\n",
        "        try:\n",
        "            data = {\n",
        "                \"model\": model_id,\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt_template.strip()\n",
        "                    }\n",
        "                ],\n",
        "                \"temperature\": 0.7,\n",
        "                \"max_tokens\": 512,\n",
        "                \"top_p\": 1.0,\n",
        "                \"frequency_penalty\": 0,\n",
        "                \"presence_penalty\": 0\n",
        "            }\n",
        "            response = requests.post(GROQ_API_URL, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            raw_output = response.json()['choices'][0]['message']['content'].strip()\n",
        "            qa_items = parse_groq_output(raw_output)\n",
        "            if qa_items:\n",
        "                return qa_items\n",
        "            else:\n",
        "                print(f\"[Attempt {retries+1}] Model output was not valid JSON or did not match schema. Retrying...\")\n",
        "\n",
        "        except (requests.exceptions.RequestException, ValidationError, json.JSONDecodeError) as e:\n",
        "            print(f\"[Attempt {retries+1}] Error: {e}. Retrying...\")\n",
        "\n",
        "        # Increment retries and throttle\n",
        "        retries += 1\n",
        "        time.sleep(THROTTLE_LIMIT)\n",
        "\n",
        "    print(\"Maximum retries reached. No valid response obtained.\")\n",
        "    return []\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. Sample Data Builders\n",
        "# -------------------------------------------------------------------\n",
        "def build_queries_and_corpus(cleaned_data: List[dict]):\n",
        "    \"\"\"\n",
        "    For each item in cleaned_data, we read 'Question' & 'Context' to build two lists:\n",
        "      - queries (List[str])\n",
        "      - corpus (List[str])\n",
        "    \"\"\"\n",
        "    queries = []\n",
        "    corpus = []\n",
        "    for item in cleaned_data:\n",
        "        q = item.get(\"Question\") or \"\"\n",
        "        c = item.get(\"Context\") or \"\"\n",
        "        queries.append(q)\n",
        "        corpus.append(c)\n",
        "    return queries, corpus\n",
        "\n",
        "\n",
        "def vanilla_tfidf_retrieval(queries: List[str], corpus: List[str]):\n",
        "    \"\"\"\n",
        "    Build a TF-IDF vectorizer on the corpus, transform each query,\n",
        "    and compute the maximum similarity score for each query.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    corpus_tfidf = vectorizer.fit_transform(corpus)  # shape=(#docs, #features)\n",
        "\n",
        "    # For demonstration, retrieve only the top doc for each query\n",
        "    # Also store which doc is top for each query\n",
        "    sim_scores_list = []\n",
        "    best_doc_indices = []\n",
        "\n",
        "    for q in queries:\n",
        "        query_vec = vectorizer.transform([q])  # shape=(1, #features)\n",
        "        scores = query_vec.dot(corpus_tfidf.T).toarray().flatten()\n",
        "        max_idx = int(np.argmax(scores))\n",
        "        max_score = float(scores[max_idx])\n",
        "        sim_scores_list.append(max_score)\n",
        "        best_doc_indices.append(max_idx)\n",
        "\n",
        "    return sim_scores_list, best_doc_indices\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Integration: Use TF-IDF, Then BEQA, Then Similarity\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Suppose 'full_data' is a dictionary with dataset_name -> List[dict], as in your code\n",
        "    # Each dict has { \"Question\": ..., \"Context\": ..., \"Response\": ... }\n",
        "\n",
        "    # Example (simplified)\n",
        "    full_data = {\n",
        "        \"squad\": [\n",
        "            {\"Question\": \"When was Python created?\",\n",
        "             \"Context\": \"Python is a language created by Guido van Rossum, first released in 1991.\",\n",
        "             \"Response\": \"1991\"},\n",
        "            {\"Question\": \"What does GIL stand for?\",\n",
        "             \"Context\": \"Python's GIL stands for Global Interpreter Lock.\",\n",
        "             \"Response\": \"Global Interpreter Lock\"}\n",
        "        ],\n",
        "        \"wiki_qa\": [\n",
        "            {\"Question\": \"Who developed Python?\",\n",
        "             \"Context\": \"Guido van Rossum developed Python.\",\n",
        "             \"Response\": \"Guido van Rossum\"}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Define your Groq credentials\n",
        "    API_KEY = \"\"\n",
        "    MODEL_ID = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "    for dataset_name, dataset_content in full_data.items():\n",
        "        print(f\"\\n=== Processing dataset: {dataset_name} ===\")\n",
        "        queries, corpus = build_queries_and_corpus(dataset_content)\n",
        "\n",
        "        # Step A: Use TF-IDF to retrieve top doc for each query\n",
        "        similarities, best_doc_indices = vanilla_tfidf_retrieval(queries, corpus)\n",
        "        print(\"Baseline TF-IDF Similarities:\", similarities)\n",
        "\n",
        "        # Step B: Apply BEQA transform to the retrieved doc for each query\n",
        "        #         Then optionally compute the similarity with the QA pairs\n",
        "        #         For demonstration, let's just transform the doc -> QA pairs,\n",
        "        #         and compute a new similarity using the QA pairs' question text.\n",
        "\n",
        "        second_stage_similarities = []\n",
        "        for i, q in enumerate(queries):\n",
        "            top_doc = corpus[best_doc_indices[i]]\n",
        "\n",
        "            # 1) Transform the doc into QA pairs\n",
        "            beqa_result = transform_with_groq(\n",
        "                context=top_doc,\n",
        "                original_query=q,\n",
        "                model_id=MODEL_ID,\n",
        "                api_key=API_KEY\n",
        "            )\n",
        "\n",
        "            # 2) (Optional) Build a 'mini-corpus' from the QA items' questions/answers\n",
        "            #    Let's pick just the 'Question' fields for a new corpus\n",
        "            beqa_questions = [qa[\"Question\"] for qa in beqa_result]\n",
        "            # If there are no QA pairs, skip\n",
        "            if not beqa_questions:\n",
        "                second_stage_similarities.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # 3) Use TF-IDF again, but now on the QA \"questions\" from the model\n",
        "            mini_vectorizer = TfidfVectorizer()\n",
        "            mini_corpus_tfidf = mini_vectorizer.fit_transform(beqa_questions)\n",
        "            query_vec = mini_vectorizer.transform([q])\n",
        "            scores = query_vec.dot(mini_corpus_tfidf.T).toarray().flatten()\n",
        "            best_score = float(np.max(scores))\n",
        "            second_stage_similarities.append(best_score)\n",
        "\n",
        "        print(\"BEQA-based Similarities:\", second_stage_similarities)\n",
        "\n",
        "        # If you want, you could also measure similarity to the \"Answer\" fields\n",
        "        # or combine them. This is flexible depending on your use case.\n",
        "\n",
        "        print (\"Ratio of Improvement:\", ratio_of_improvement(second_stage_similarities, similarities))"
      ],
      "metadata": {
        "id": "XLtDqaQL77U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Extension to Custom Datasets and Retrievers**\n",
        "\n",
        "Below is a concise set of instructions for **extending** the code to a custom dataset (instead of the simple `full_data` dictionary) and a custom retriever (instead of the current TF-IDF pipeline).\n",
        "\n",
        "---\n",
        "\n",
        "## Extending to a Custom Dataset\n",
        "\n",
        "### A. Load Your Dataset\n",
        "\n",
        "1. **Data Format**: Suppose your dataset is stored in a CSV, JSON, or is fetched from an API. You want to produce a list of dictionaries like:\n",
        "   ```python\n",
        "   [\n",
        "     {\"Question\": \"some question\", \"Context\": \"some context\", \"Response\": \"some response\"},\n",
        "     ...\n",
        "   ]\n",
        "   ```\n",
        "   at the end of your loading/parsing process.\n",
        "\n",
        "2. **Write a Loader/Parsing Function**:\n",
        "   - If your dataset is in CSV:\n",
        "     ```python\n",
        "     import csv\n",
        "\n",
        "     def load_my_dataset(csv_file_path: str):\n",
        "         data = []\n",
        "         with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
        "             reader = csv.DictReader(f)\n",
        "             for row in reader:\n",
        "                 data.append({\n",
        "                     \"Question\": row.get(\"QuestionColumn\", \"\"),\n",
        "                     \"Context\": row.get(\"ContextColumn\", \"\"),\n",
        "                     \"Response\": row.get(\"ResponseColumn\", \"\")\n",
        "                 })\n",
        "         return data\n",
        "     ```\n",
        "   - If your dataset is in JSON, adapt similarly with `json.load(...)`.\n",
        "\n",
        "3. **Feed the Loaded Data into the Existing Pipeline**:\n",
        "   - You can then place the loaded data into your `full_data` dictionary, e.g.:\n",
        "     ```python\n",
        "     my_data = load_my_dataset(\"my_questions.csv\")\n",
        "     full_data = {\"my_dataset_name\": my_data}\n",
        "     ```\n",
        "   - The rest of your code will handle the new dataset in the same loop where it processes `full_data`.\n",
        "\n",
        "### B. Integrate with `build_queries_and_corpus`\n",
        "\n",
        "- Once you have the list of dictionaries from your loader, you can call:\n",
        "  ```python\n",
        "  queries, corpus = build_queries_and_corpus(my_data)\n",
        "  ```\n",
        "- Then proceed with your retrieval+BEQA pipeline exactly as shown in the `if __name__ == '__main__':` section.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Extending to a Custom Retriever\n",
        "\n",
        "The current code uses a **vanilla TF-IDF** retriever, defined by:\n",
        "\n",
        "```python\n",
        "def vanilla_tfidf_retrieval(queries: List[str], corpus: List[str]):\n",
        "    # ...\n",
        "    return sim_scores_list, best_doc_indices\n",
        "```\n",
        "\n",
        "### A. Creating a New Retriever\n",
        "\n",
        "1. **Implementation**: Suppose you want to replace TF-IDF with a custom approach (BM25, DPR, or a specialized neural model). You need a function with the same interface:\n",
        "\n",
        "   ```python\n",
        "   def my_custom_retriever(queries: List[str], corpus: List[str]):\n",
        "       \"\"\"\n",
        "       Accepts queries and a corpus of documents.\n",
        "       Returns:\n",
        "         sim_scores_list: a list of floats representing, for each query,\n",
        "                          the highest similarity to a corpus doc.\n",
        "         best_doc_indices: a list of ints where best_doc_indices[i] is the index\n",
        "                           of the best doc for queries[i].\n",
        "       \"\"\"\n",
        "       # (1) Build your retrieval system on 'corpus'\n",
        "       # (2) For each query, find the doc with the highest similarity\n",
        "       # ...\n",
        "       return sim_scores_list, best_doc_indices\n",
        "   ```\n",
        "\n",
        "2. **Example:**\n",
        "   - If you’re using **BM25** from `rank_bm25`, you’d do something like:\n",
        "     ```python\n",
        "     from rank_bm25 import BM25Okapi\n",
        "\n",
        "     def my_custom_retriever(queries, corpus):\n",
        "         tokenized_corpus = [doc.split() for doc in corpus]\n",
        "         bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "         sim_scores_list = []\n",
        "         best_doc_indices = []\n",
        "         for q in queries:\n",
        "             tokenized_q = q.split()\n",
        "             scores = bm25.get_scores(tokenized_q)\n",
        "             max_idx = int(np.argmax(scores))\n",
        "             max_score = float(scores[max_idx])\n",
        "             sim_scores_list.append(max_score)\n",
        "             best_doc_indices.append(max_idx)\n",
        "\n",
        "         return sim_scores_list, best_doc_indices\n",
        "     ```\n",
        "   - If you’re using a neural approach, e.g., **Sentence-BERT**:\n",
        "     ```python\n",
        "     from sentence_transformers import SentenceTransformer\n",
        "     import numpy as np\n",
        "\n",
        "     def my_custom_retriever(queries, corpus):\n",
        "         model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "         corpus_embeddings = model.encode(corpus, convert_to_numpy=True)\n",
        "         query_embeddings = model.encode(queries, convert_to_numpy=True)\n",
        "\n",
        "         sim_scores_list = []\n",
        "         best_doc_indices = []\n",
        "         for i, q_emb in enumerate(query_embeddings):\n",
        "             # Compute dot product or cosine similarity\n",
        "             scores = np.dot(corpus_embeddings, q_emb)  # shape=(#docs,)\n",
        "             max_idx = np.argmax(scores)\n",
        "             max_score = float(scores[max_idx])\n",
        "             sim_scores_list.append(max_score)\n",
        "             best_doc_indices.append(max_idx)\n",
        "\n",
        "         return sim_scores_list, best_doc_indices\n",
        "     ```\n",
        "   - Adjust the similarity measure to your preference.\n",
        "\n",
        "### B. Plug the New Retriever into the Pipeline\n",
        "\n",
        "In the main code, instead of:\n",
        "\n",
        "```python\n",
        "similarities, best_doc_indices = vanilla_tfidf_retrieval(queries, corpus)\n",
        "```\n",
        "\n",
        "…just call:\n",
        "\n",
        "```python\n",
        "similarities, best_doc_indices = my_custom_retriever(queries, corpus)\n",
        "```\n",
        "\n",
        "Everything else (the **BEQA transform** and second-stage similarity) will remain the same—because your new retriever function still provides:\n",
        "\n",
        "- `similarities`: top similarity scores for each query,\n",
        "- `best_doc_indices`: index of the top document in `corpus` for each query.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Final Example Putting It All Together\n",
        "\n",
        "```python\n",
        "if __name__ == '__main__':\n",
        "    # 1) Load the dataset from a CSV\n",
        "    my_data = load_my_dataset(\"my_questions.csv\")   # implement your CSV loader\n",
        "    full_data = {\"my_dataset\": my_data}\n",
        "\n",
        "    # 2) Use custom retriever\n",
        "    for dataset_name, dataset_content in full_data.items():\n",
        "        queries, corpus = build_queries_and_corpus(dataset_content)\n",
        "        \n",
        "        # Replace with your own function:\n",
        "        similarities, best_doc_indices = my_custom_retriever(queries, corpus)\n",
        "\n",
        "        # Then do the same transform + second-stage similarity as you do now\n",
        "        # ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Summary**:\n",
        "\n",
        "1. **Custom Dataset**  \n",
        "   - Write a loader that yields a list of dicts \\([{\"Question\": Q, \"Context\": C, \"Response\": R}, ...]\\).  \n",
        "   - Integrate that list into your pipeline by calling `build_queries_and_corpus`.\n",
        "\n",
        "2. **Custom Retriever**  \n",
        "   - Implement a function that returns `(sim_scores_list, best_doc_indices)` from your chosen retrieval logic.  \n",
        "   - Replace `vanilla_tfidf_retrieval` calls with your custom function.\n",
        "\n",
        "Everything else—BEQA transforms, second-phase similarity, ratio-of-improvement calculations—remains the same. This approach ensures a **pluggable** architecture where you can swap out or expand the dataset source and the retrieval method."
      ],
      "metadata": {
        "id": "WpGQBVjP9wiM"
      }
    }
  ]
}
